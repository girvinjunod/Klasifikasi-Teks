{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2UYsTiD1Kxz3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CuZ1mfjtKxgX"
      },
      "outputs": [],
      "source": [
        "# dataTrain = 'https://drive.google.com/file/d/1s9koNFTgraCMbx0P2V-L_ogvA3map9PD/view?usp=sharing' # copy of original train dataset, in personal drive\n",
        "# dataTest = 'https://drive.google.com/file/d/1d0zLt8IG2Icqmw4jwURPaDhkiWEsVn3n/view?usp=sharing' # copy of original test dataset, in personal drive\n",
        "# dataTrainPath = 'https://drive.google.com/uc?export=download&id='+dataTrain.split('/')[-2]\n",
        "# dataTestPath = 'https://drive.google.com/uc?export=download&id='+dataTest.split('/')[-2]\n",
        "path = \"../../data_worthcheck/\"\n",
        "dataTrain = pd.read_csv(path + \"train.csv\")\n",
        "dataTest = pd.read_csv(path + \"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8SAvLjDjKR4Z"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import tensorflow as tf\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ThjdPvVPLeaQ"
      },
      "outputs": [],
      "source": [
        "X_train = dataTrain['text_a'].values\n",
        "y_train = dataTrain['label']\n",
        "\n",
        "X_test = dataTest['text_a'].values\n",
        "y_test = dataTest['label']\n",
        "\n",
        "maps = {\"no\": 0, \"yes\": 1}\n",
        "y_train = y_train.replace(maps)\n",
        "y_test = y_test.replace(maps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd4QAPvPKfKQ",
        "outputId": "e1732b44-9524-4d6f-b074-321d4972d7ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mas piyuuu mugo2 corona tuh mulut tersumpal ma corona\n",
            "[158, 823, 1, 129, 1031, 520, 1]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "Xcnn_train = tokenizer.texts_to_sequences(X_train)\n",
        "Xcnn_test = tokenizer.texts_to_sequences(X_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1  \n",
        "print(X_train[1])\n",
        "print(Xcnn_train[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gqkLAmKIKhGv"
      },
      "outputs": [],
      "source": [
        "# Padding the data samples to a maximum review length in words\n",
        "max_words = 450\n",
        "Xcnn_train = sequence.pad_sequences(Xcnn_train, maxlen=max_words,  padding='post')\n",
        "Xcnn_test = sequence.pad_sequences(Xcnn_test, maxlen=max_words,  padding='post')\n",
        "# Building the CNN Model\n",
        "model = Sequential()      # initilaizing Sequential for CNN \n",
        "# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output \n",
        "model.add(Embedding(vocab_size, 32, input_length=max_words))\n",
        "# Conv Layer\n",
        "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
        "# Pooling Layer\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Flatten())\n",
        "# Dense Layer\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsKNvA3FKidQ",
        "outputId": "70cc8cdd-4c19-4e0c-e584-65da2937bede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 450, 32)           1482080   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 450, 32)           3104      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 225, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 7200)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1843456   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,328,897\n",
            "Trainable params: 3,328,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Optimize with SGD\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.1), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G15a8jEHN09",
        "outputId": "4ae85c50-5fba-4431-d8dc-150d0c973d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.908060\n",
            "Testing Accuracy:  0.853929\n",
            "F1 score: 0.747500\n"
          ]
        }
      ],
      "source": [
        "# Fitting the model and search f1 score\n",
        "from sklearn.metrics import f1_score\n",
        "model.fit(Xcnn_train, y_train,\n",
        "                    epochs=3,\n",
        "                    verbose=False,\n",
        "                    validation_data=(Xcnn_test, y_test),\n",
        "                    batch_size=16)\n",
        "loss, accuracy = model.evaluate(Xcnn_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.6f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(Xcnn_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.6f}\".format(accuracy)) \n",
        "\n",
        "\n",
        "yhat_probs = model.predict(Xcnn_test, verbose=0)\n",
        "predict_y = model.predict(Xcnn_test, verbose=0)\n",
        "yhat_classes=np.argmax(predict_y,axis=1)\n",
        "yhat_probs = yhat_probs[:, 0]\n",
        " \n",
        "\n",
        "f1 = f1_score(y_test, yhat_classes, average='micro')\n",
        "print('F1 score: %f' % f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "438727c8858bc46e257ba69863512eb04c8e9f5db9a1c7102296af502b4092eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
